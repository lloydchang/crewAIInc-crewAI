# config/tools.yaml

tedx_search:
  llm_config:
    provider: "ollama"
    config:
      model: "ollama/llama3"
      temperature: 0
  embedder_config:
    provider: "ollama"
    config:
      model: "nomic-embed-text"
      temperature: 0
  data_path: "data/github-mauropelucchi-tedx_dataset-update_2024-details.csv"

tedx_slug:
  llm_config:
    provider: "ollama"
    config:
      model: "ollama/llama3"
      temperature: 0
  embedder_config:
    provider: "ollama"
    config:
      model: "nomic-embed-text"
      temperature: 0
  data_path: "data/github-mauropelucchi-tedx_dataset-update_2024-details.csv"

tedx_transcript:
  llm_config:
    provider: "ollama"
    config:
      model: "ollama/llama3"
      temperature: 0
  embedder_config:
    provider: "ollama"
    config:
      model: "nomic-embed-text"
      temperature: 0
  data_path: "data/github-mauropelucchi-tedx_dataset-update_2024-details.csv"

duckduckgo_search:
  llm_config:
    provider: "ollama"
    config:
      model: "ollama/llama3"
      temperature: 0
  embedder_config:
    provider: "ollama"
    config:
      model: "nomic-embed-text"
      temperature: 0

sdg_align:
  llm_config:
    provider: "ollama"
    config:
      model: "ollama/llama3"
      temperature: 0
  embedder_config:
    provider: "ollama"
    config:
      model: "nomic-embed-text"
      temperature: 0
  data_path: "data/sdg_data.csv"

sustainability_impact:
  llm_config:
    provider: "ollama"
    config:
      model: "ollama/llama3"
      temperature: 0
  embedder_config:
    provider: "ollama"
    config:
      model: "nomic-embed-text"
      temperature: 0
  data_path: "data/impact_data.csv"
